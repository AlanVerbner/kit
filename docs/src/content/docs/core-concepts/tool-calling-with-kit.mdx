---
title: Tool-Calling with kit
subtitle: Practical patterns for letting your LLM drive kit's code-intelligence APIs
---

Modern LLM runtimes (OpenAI *function-calling*, Anthropic *tools*, etc.) let you hand the model a **menu of functions** in JSON-Schema form. The model then plans its own calls – no hard-coded if/else trees required. 

With `kit` you can  expose its code-intelligence primitives (search, symbol extraction, summaries, etc) and let the LLM decide which one to execute in each turn.  Your app stays declarative: *"Here are the tools, here is the user's question, please help."*

In practice that means you can drop `kit` into an existing chat agent with **zero orchestration logic** beyond registering the schema.  The rest of this guide shows the small amount of glue code needed and the conversational patterns that emerge.

You may also be interested in kit's [MCP integration](../mcp/using-kit-with-mcp), which can achieve similar goals.

`kit` exposes its code-intelligence primitives as **callable tools**.  Inside Python you can grab the JSON-Schema list with a single helper (`kit.get_tool_schemas()`) and hand that straight to your LLM runtime.  Once the schema is registered the model can decide _when_ to call:

* `open_repository`
* `search_code`
* `extract_symbols`
* `find_symbol_usages`
* `get_file_tree`, `get_file_content`
* `get_code_summary`

This page shows the minimal JSON you need, the decision patterns the model will follow, and a multi-turn example.

## 1  Register the tools

### OpenAI Chat Completions

```python
from openai import OpenAI
from kit import get_tool_schemas

client = OpenAI()

# JSON-Schema for every kit tool
functions = get_tool_schemas()

messages = [
    {"role": "system", "content": "You are an AI software engineer, some refer to as the 'Scottie Scheffler of Programming'. Feel free to call tools when you need context."},
]
```

`functions` is a list of JSON-Schema objects. Pass it directly as the `tools`/`functions` parameter to `client.chat.completions.create()`.

### Anthropic (messages-v2)

```python
from anthropic import Anthropic
anthropic = Anthropic()

# JSON-Schema for every kit tool
functions = get_tool_schemas()

response = anthropic.messages.create(
    model="claude-3-7-sonnet-20250219",
    system="You are an AI software engineer…",
    tools=functions,
    messages=[{"role": "user", "content": "I got a test failure around FooBar.  Help me."}],
)
```

## 2  When should the model call which tool?

Below is the heuristic kit's own prompts (and our internal dataset) encourage.  You **don't** need to hard-code this logic—the LLM will pick it up from the tool names / descriptions—but understanding the flow helps you craft better conversation instructions.

| Situation | Suggested tool(s) |
|-----------|-------------------|
| No repo open yet | `open_repository` (first turn) |
| "What files mention X?" | `search_code` (fast regex) |
| "Show me the function/class definition" | `get_file_content` *or* `extract_symbols` |
| "Where else is `my_func` used?" | 1) `extract_symbols` (file-level) → 2) `find_symbol_usages` |
| "Summarize this file/function for the PR description" | `get_code_summary` |
| IDE-like navigation | `get_file_tree` + `get_file_content` |

A **typical multi-turn session**:

```
User: I keep getting KeyError("user_id") in prod.

Assistant (tool call): search_code {"repo_id": "42", "query": "KeyError(\"user_id\")"}

Tool result → 3 hits returned (files + line numbers)

Assistant: The error originates in `auth/session.py` line 88.  Shall I show you that code?

User: yes, show me.

Assistant (tool call): get_file_content {"repo_id": "42", "file_path": "auth/session.py"}

Tool result → file text

Assistant: Here is the snippet … (explanatory text)
```

## 3  Prompt orchestration: system / developer messages

Tool-calling conversations have **three channels of intent**:

1. **System prompt** – your immutable instructions (e.g. *"You are an AI software-engineer agent."*)  
2. **Developer prompt** – _app-level_ steering: *"If the user asks for code you have not seen, call `get_file_content` first."*  
3. **User prompt** – the human's actual message.

`kit` does *not* impose a format—you simply include the JSON-schema from `kit.get_tool_schemas()` in your `tools` / `functions`